# configs/dqn_enhanced.yaml
# Configuration for the Enhanced DQN agent (Double + Dueling)
agent_name: dqn_enhanced
algo: dqn

# -------------------
# Model parameters
# -------------------
dueling: true                 # Use Dueling architecture (separate value + advantage streams)

# -------------------
# Training parameters
# -------------------
optimizer: Adam               # Optimizer used for training
learning_rate: 0.0001         # Step size for gradient updates
gamma: 0.99                   # Discount factor for future rewards
batch_size: 32                # Number of transitions sampled per training step
replay_buffer_size: 1000000   # Capacity of replay buffer (transitions stored)
target_update_freq: 10000     # Frequency (in gradient updates) to sync target network
gradient_clip_norm: 10.0      # Max global norm for gradient clipping
huber_loss_delta: 1.0         # Delta parameter for Huber loss (robust to outliers)

# -------------------
# Q-Learning parameters
# -------------------
double_q: true                # Use Double Q-learning (reduces overestimation bias)

# -------------------
# Exploration parameters
# -------------------
epsilon_start: 1.0            # Initial epsilon (100% random actions)
epsilon_end: 0.1              # Final epsilon (min exploration rate)
epsilon_decay_frames: 1000000 # Number of frames over which epsilon decays linearly
learning_starts: 50000        # Frames collected before starting training (fill buffer)

# -------------------
# Evaluation & Checkpointing
# -------------------
eval_freq_frames: 25000       # How often (in frames) to run evaluation + checkpoint
eval_episodes: 30             # Number of episodes to average per evaluation
